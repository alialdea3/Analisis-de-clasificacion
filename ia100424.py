# -*- coding: utf-8 -*-
"""ia100424.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mp08zb2Qud7TIutcsXtbHhXqCaahTbcB
"""

import pandas as pd

from matplotlib import pyplot as plt
from sklearn.model_selection import train_test_split

from sklearn.neighbors import KNeighborsClassifier

from sklearn.metrics import accuracy_score

#queremos eliminar el indice
X=pd.read_csv('./penguin-dataset-preprocessed.csv').iloc[:,1:]
Y=pd.read_csv('./penguin-dataset-Y.csv').iloc[:,1:]

X.head()

Y.head()

plt.scatter(X.iloc[:,0], X.iloc[:,1])
plt.show()

X.shape, Y.shape

#X es de entrenamiento y x es de test
X, x, Y, y = train_test_split(X,Y, test_size=0.3, random_state=42)
X.shape, x.shape, Y.shape, y.shape

#lo inicializamos y le pasamos los vecinos
model= KNeighborsClassifier(n_neighbors=5)

model.fit(X,Y.values.ravel())#y el values lo pasa a numpy el ravel elimina la segunda dimension
#fase de entrenamiento hecha

#ahora hacemos las predicciones no necesitamos la Y porque es lo que queremos predecir
y_pred= model.predict(x)

y_pred

#para asegurarnos de si la predicion es buena o mala comparamos con nuestra Y ya que la tenemos
y.values.ravel()
#siempre habra algun error, como no es optimo ir mirando uno a uno podemos hacer un grafico

accuracy_score(y,y_pred)
#vemos que tiene una muy buena prediccion

#ahora queremos averiguar cual es el valor optimo de k, ya que esta vez ha ido bien
best_k=0
best_accuracy=0
best_neigh= None

accuracies = []

for j in range (1,100):
  neigh= KNeighborsClassifier(n_neighbors=j).fit(X,Y.values.ravel())
  y_pred= neigh.predict(x)

  accuracies.append(accuracy_score(y,y_pred))

  if accuracy_score(y, y_pred)>best_accuracy:
    best_k=j+1
    best_accuracy=accuracy_score(y,y_pred)
    best_neigh=neigh

y_pred=best_neigh.predict(x)

plt.plot(accuracies)
plt.show()
#en caso de empate coger la k menor por el coste computacional